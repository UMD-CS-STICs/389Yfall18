{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Neural Networks\n",
    "Machine learning is a different aproach to writting programs. Traditional programs are written as a series of steps that explicitly solve the problem. This approach works very well for problems that are easily described by mathematical models. For example, storing ordered data in a binary tree is a problem that is well suited for traditional algorithms. Trees follow simple rules that allow you to easily reason about their behaviour. More complex problems however, for example facial recognition, are extremely difficult to solve this way. There are no simple rules or simplifying assumptions you can make that allow you to easily tell pictures of faces from non-faces. Machine learning takes a different aproach. Instead of explicitely modeling the solution, Machine learning imposes restrictions on what form the solution can take and lets the solution emerge from that restrited model on its own. In the case of Neural Neworks this \"restriction\" process amounts to choosing which neurons are connected where and the solution \"emerges\" as the network learns from examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Perceptron\n",
    "![](https://cdn-images-1.medium.com/max/800/1*nRRXhhjSjKNpGn-T3yF2Ew.jpeg)\n",
    "A perceptron (pictured above) is the simplest kind of nueral network. It recieved $n$ inputs each with an associated weight $w$. These inputs are multiplied by their respective weights, summed together, and then passed through some nonlinear activation function. It is also common to add a constant \"bias\" term to the sum before passing it through the activation function. This bias can be convieniently represented by connecting an extra input to the perceptron that always has a value of \"1\". For the simple perceptron, this activation function is a unit step function. \n",
    "\n",
    "A full mathematical description of the perception can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "0 & \\sum x_iw_i \\leq 0 \\\\\n",
    "1 & \\sum x_iw_i \\ge 0\n",
    "\\end{cases}\n",
    "\\end{gather}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Translate the above function to python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perceptron(x, w):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the block below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([.1,-.2,.3,-.4])\n",
    "x1 = np.array([1, 2, 3, 4])\n",
    "x2 = np.array([20, 2, 3, 4])\n",
    "x3 = np.array([1, 2, 3, 8])\n",
    "x4 = np.array([1, 2, 8, 4])\n",
    "assert perceptron(x1, w) == 0\n",
    "assert perceptron(x2, w) == 1\n",
    "assert perceptron(x3, w) == 0\n",
    "assert perceptron(x4, w) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Surfaces\n",
    "Graphs can help vizualize the decision surfaces of classification algorithms (like perceptrons). The one below vizualizes a perceptron with two inputs, the x-axis being the first input and the y-axis being the second input. The blue line is the decision boundry or \"surface\". Everything above the line is in class \"1\" and everything below is in class \"0\". I've also graphed four points that I want the peceptron to be able to classify. The red dot should be in class \"1\" while the green dots should be in class \"0\".\n",
    "\n",
    "The position of the decision boundry is determined by the weight vector $w$. The first and second entries in $w$ are the weights for the first and second inputs to the perceptron respectively, while the third weight represents the bias term. See if you can play around with the entries of $w$ until the perceptron classifies all of the graphed points correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: play around with the entries of w until you get the desired result!\n",
    "w = np.array([0.1, -0.1, -0.05])\n",
    "\n",
    "# this code is for graphing\n",
    "x1 = np.arange(-2, 2, 0.1)\n",
    "plt.plot(x1, (-w[0]*x1 - w[2]) / w[1])\n",
    "plt.plot([1],[1], 'ro', markersize=20)\n",
    "plt.plot([0],[1], 'go', markersize=20)\n",
    "plt.plot([1],[0], 'go', markersize=20)\n",
    "plt.plot([0],[0], 'go', markersize=20)\n",
    "plt.xlim([-0.2, 1.2])\n",
    "plt.ylim([-0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you have tweaked w, you should pass the below test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the third parameter is always 1 for the bias term\n",
    "assert perceptron(np.array([0, 0, 1]), w) == 0\n",
    "assert perceptron(np.array([0, 1, 1]), w) == 0\n",
    "assert perceptron(np.array([1, 0, 1]), w) == 0\n",
    "assert perceptron(np.array([1, 1, 1]), w) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Learning Rule\n",
    "While fiddling with weights by hand is a good exercise, this of course is never done in practice. Instead we use a learning rule and examples to train our network. \n",
    "\n",
    "The perceptron learning rule is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "    w_{t+1}^{(i)} = w_t^{(i)} + r  (d - y)x_t^{(i)}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $w_{t}^{(i)}$ is the $i$th weight at time $t$, $d$ is the desired output of the perceptron, $y$ is the actual output of the perceptron, and $x_t^{(i)}$ is the $i$th input to the perceptron at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Fill in the code below to make the perceptron learn on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: fill in the update_weight function\n",
    "def update_weight(w, r, d, y, x):\n",
    "    return 0\n",
    "\n",
    "# define our training examples. Each row is an example\n",
    "examples = np.array([[0, 0],\n",
    "                     [1, 0],\n",
    "                     [0, 1],\n",
    "                     [1, 1]])\n",
    "\n",
    "# define the desired target class of each example\n",
    "targets = np.array([0, 0, 0, 1])\n",
    "\n",
    "# define an empty weight vector \n",
    "w = np.empty(3)\n",
    "print(\"Original w: {}\".format(w))\n",
    "\n",
    "# define the learning rate\n",
    "r = 0.005\n",
    "\n",
    "for _ in range(1000):\n",
    "    # pick a random example\n",
    "    i = np.random.randint(4)\n",
    "    \n",
    "    # concatenate a 1 to the end of the input for the bias\n",
    "    x = np.hstack((examples[i], [1]))\n",
    "    \n",
    "    # pass the input through the perceptron\n",
    "    result = perceptron(x, w)\n",
    "    \n",
    "    # Todo: update each weight with the update weight function\n",
    "    \n",
    "print(\"After training w: {}\".format(w))\n",
    "\n",
    "# make sure the perceptron learned correctly \n",
    "assert perceptron(np.array([0, 0, 1]), w) == 0\n",
    "assert perceptron(np.array([0, 1, 1]), w) == 0\n",
    "assert perceptron(np.array([1, 0, 1]), w) == 0\n",
    "assert perceptron(np.array([1, 1, 1]), w) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a graph of the learnt decision surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(-2, 2, 0.1)\n",
    "plt.plot(x1, (-w[0]*x1 - w[2]) / w[1])\n",
    "plt.plot([1],[1], 'ro', markersize=20)\n",
    "plt.plot([0],[1], 'go', markersize=20)\n",
    "plt.plot([1],[0], 'go', markersize=20)\n",
    "plt.plot([0],[0], 'go', markersize=20)\n",
    "plt.xlim([-0.2, 1.2])\n",
    "plt.ylim([-0.2, 1.2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
